import os
import shutil
import tempfile
import numpy as np
import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.evaluation import evaluate_policy
from rl_env import TradingEnv

# ----------------------------
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ (–º–µ–Ω—è–π –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
# ----------------------------
BEST_MODEL_PATH = "./best_model/best_model.zip"      # <-- –¥–æ–ª–∂–Ω–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å –∑–∞—Ä–∞–Ω–µ–µ
TMP_MODEL_PATH  = "./best_model/_candidate_model.zip"
BACKUP_MODEL_PATH = "./best_model/_best_backup.zip"
TRAIN_CSV = "archive/M15/BTCUSDT_M15.csv"
TRAIN_SPLIT = 0.8
SEED = 45
N_EVAL_EPISODES = 3     # —Å–∫–æ–ª—å–∫–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (–±–æ–ª—å—à–µ ‚Äî –±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)
TOTAL_TIMESTEPS = 100_000
N_ITERATIONS = 100        # —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å —Ü–∏–∫–ª "train->eval->compare"
VERBOSE = 0

# ----------------------------
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ----------------------------
def make_vec_env_from_df(df, seed=0):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DummyVecEnv –æ–±—ë—Ä–Ω—É—Ç—ã–π Monitor'–æ–º."""
    def _init():
        return Monitor(TradingEnv(df))
    env = DummyVecEnv([_init])
    env.seed(seed)
    return env

def evaluate(model, eval_env, n_eval_episodes=N_EVAL_EPISODES):
    """
    –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ eval_env.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç mean_reward, std_reward.
    """
    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=n_eval_episodes, deterministic=True, render=False)
    return mean_reward, std_reward

def atomic_replace(src_path, dst_path):
    """
    –ê—Ç–æ–º–∞—Ä–Ω–∞—è –∑–∞–º–µ–Ω–∞ —Ñ–∞–π–ª–∞: –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ, –∑–∞—Ç–µ–º os.replace.
    """
    os.replace(src_path, dst_path)

# ----------------------------
# –ì–ª–∞–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
# ----------------------------
def main(
    best_model_path=BEST_MODEL_PATH,
    train_csv=TRAIN_CSV,
    train_split=TRAIN_SPLIT,
    total_timesteps=TOTAL_TIMESTEPS,
    n_iterations=N_ITERATIONS,
    n_eval_episodes=N_EVAL_EPISODES,
    seed=SEED
):
    # –ü—Ä–æ–≤–µ—Ä–∫–∏
    if not os.path.exists(best_model_path):
        raise FileNotFoundError(f"–õ—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ –ø—É—Ç–∏ '{best_model_path}'. –ü–æ–º–µ—Å—Ç–∏—Ç–µ —Ñ–∞–π–ª –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –∑–∞–ø—É—Å–∫.")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
    df = pd.read_csv(train_csv)
    split_index = int(len(df) * train_split)
    train_df = df[:split_index].reset_index(drop=True)
    val_df   = df[split_index:].reset_index(drop=True)

    # –°–æ–∑–¥–∞—ë–º env'—ã
    train_env = make_vec_env_from_df(train_df, seed=seed)
    eval_env  = make_vec_env_from_df(val_df, seed=seed+1)  # –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–∏–¥ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–¥–Ω—É –ª—É—á—à—É—é –º–æ–¥–µ–ª—å ‚Äî **–æ–¥–∏–Ω —Ä–∞–∑** (–Ω–æ –º—ã –±—É–¥–µ–º –ø–æ–¥–º–µ–Ω—è—Ç—å –µ—ë –≤ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ rollback/save)
    print("üîÅ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑...")
    model = PPO.load(best_model_path, env=train_env)   # —Å—Ä–∞–∑—É –ø—Ä–∏–≤—è–∑—ã–≤–∞–µ–º –∫ train_env
    model.set_random_seed(seed)

    # –°–æ—Ö—Ä–∞–Ω–∏–º –∫–æ–ø–∏—é –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–ª—É—á–∞–π –æ—Ç–∫–∞—Ç–∞
    shutil.copy2(best_model_path, BACKUP_MODEL_PATH)

    # 1) –û—Ü–µ–Ω–∏–º baseline-reward **—Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑**:
    print("üìä –û—Ü–µ–Ω–∏–≤–∞–µ–º baseline (initial) reward –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–æ–¥–∏–Ω –∑–∞–ø—É—Å–∫)...")
    mean_reward_best, std_reward_best = evaluate(model, eval_env, n_eval_episodes=n_eval_episodes)
    print(f"Baseline mean reward: {mean_reward_best:.6f} ¬± {std_reward_best:.6f}")

    # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–∏–ª–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:
    # –°—á–∏—Ç–∞–µ–º —É–ª—É—á—à–µ–Ω–∏–µ Œî = R_new - R_best.
    # –ï—Å–ª–∏ Œî > 0  => –Ω–æ–≤–æ–µ –ª—É—á—à–µ => —Å–æ—Ö—Ä–∞–Ω—è–µ–º.
    # (–ú–æ–∂–Ω–æ –ø–æ–º–µ–Ω—è—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–π –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç, –Ω–æ —Å–µ–π—á–∞—Å –ø—Ä–æ—Å—Ç–æ–π –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π.)
    # –§–æ—Ä–º—É–ª–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ —É–ª—É—á—à–µ–Ω–∏—è: 100 * Œî / |R_best|  (–µ—Å–ª–∏ R_best != 0).

    for it in range(1, n_iterations + 1):
        print("\n" + "="*60)
        print(f"–ò—Ç–µ—Ä–∞—Ü–∏—è {it}/{n_iterations}: train {total_timesteps} —à–∞–≥–æ–≤")
        print("="*60)

        # 2) Train: —Å–æ–∑–¥–∞—ë–º candidate –∫–æ–ø–∏—é –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ –æ–±—É—á–µ–Ω–∏—è
        #    (–º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º .save/.load –¥–ª—è –∞—Ç–æ–º–∞—Ä–Ω–æ—Å—Ç–∏)
        # –°–æ—Ö—Ä–∞–Ω–∏–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª (—á—Ç–æ–±—ã –∏–º–µ—Ç—å candidate —Å—Ç–∞—Ä—Ç–æ–≤—É—é —Ç–æ—á–∫—É)
        candidate_start_path = TMP_MODEL_PATH
        print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—Ç–∞—Ä—Ç–æ–≤–æ–π —Ç–æ—á–∫–∏ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è...")
        model.save(candidate_start_path)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º candidate (–Ω–æ–≤—ã–π –æ–±—ä–µ–∫—Ç) –∏ –ø—Ä–∏–≤—è–∑—ã–≤–∞–µ–º train_env
        candidate = PPO.load(candidate_start_path, env=train_env,verbose = 0)
        candidate.set_random_seed(seed + it)  # –Ω–µ–º–Ω–æ–≥–æ –∏–∑–º–µ–Ω–∏—Ç—å —Å–∏–¥ –¥–ª—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è

        # 3) –û–±—É—á–∞–µ–º candidate
        print(f"‚öôÔ∏è  –û–±—É—á–µ–Ω–∏–µ candidate –º–æ–¥–µ–ª–∏: {total_timesteps} —à–∞–≥–æ–≤...")
        candidate.learn(total_timesteps=total_timesteps, reset_num_timesteps=False, progress_bar=True)

        # 4) –û—Ü–µ–Ω–∏–≤–∞–µ–º candidate –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ‚Äî **–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞** –≤ —ç—Ç–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
        print("üìà –û—Ü–µ–Ω–∏–≤–∞–µ–º candidate –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏...")
        mean_reward_candidate, std_reward_candidate = evaluate(candidate, eval_env, n_eval_episodes=n_eval_episodes)
        print(f"Candidate mean reward: {mean_reward_candidate:.6f} ¬± {std_reward_candidate:.6f}")

        # 5) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
        delta = mean_reward_candidate - mean_reward_best
        # –ø—Ä–æ—Ü–µ–Ω—Ç —É–ª—É—á—à–µ–Ω–∏—è (–∑–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å)
        pct = (100.0 * delta / abs(mean_reward_best)) if mean_reward_best != 0 else float('inf') if delta>0 else -float('inf')

        print(f"Œî = R_candidate - R_best = {delta:.6f} ( {pct:.4f}% )")

        if delta > 0:
            # candidate –ª—É—á—à–µ ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –Ω–æ–≤—ã–π –ª—É—á—à–∏–π (–∞—Ç–æ–º–∞—Ä–Ω–æ –∑–∞–º–µ–Ω—è–µ–º —Ñ–∞–π–ª)
            print("üèÜ Candidate –ª—É—á—à–µ. –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –∫–∞–∫ –Ω–æ–≤—É—é –ª—É—á—à—É—é –º–æ–¥–µ–ª—å...")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ –≤ —Ñ–∞–π–ª, –∑–∞—Ç–µ–º –∞—Ç–æ–º–∞—Ä–Ω–æ replace
            candidate.save(TMP_MODEL_PATH)
            atomic_replace(TMP_MODEL_PATH, best_model_path)
            # –æ–±–Ω–æ–≤–∏–º in-memory –º–æ–¥–µ–ª—å –∏ best reward
            model = PPO.load(best_model_path, env=train_env)
            mean_reward_best = mean_reward_candidate
            std_reward_best = std_reward_candidate
            print(f"‚úÖ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_model_path}")
        else:
            # candidate –Ω–µ –ª—É—á—à–µ ‚Äî –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º; –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∞—Ä—É—é –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç–∏
            print("‚ùå Candidate —Ö—É–∂–µ –∏–ª–∏ —Ä–∞–≤–µ–Ω. –û—Ç–∫–∞—Ç—ã–≤–∞–µ–º—Å—è –∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç–∏ (–±–µ–∑ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞).")
            model = PPO.load(best_model_path, env=train_env)
            # mean_reward_best –æ—Å—Ç–∞—ë—Ç—Å—è –ø—Ä–µ–∂–Ω–∏–º

        # –∫—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ –∏—Ç–µ—Ä–∞—Ü–∏–∏
        print(f"–ò—Ç–µ—Ä–∞—Ü–∏—è {it} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –¢–µ–∫—É—â–∏–π best mean reward = {mean_reward_best:.6f}")

    # –ò—Ç–æ–≥
    print("\n" + "="*40)
    print("–ì–æ—Ç–æ–≤–æ. –§–∏–Ω–∞–ª—å–Ω–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å:")
    print(f" -> path: {best_model_path}")
    print(f" -> mean reward: {mean_reward_best:.6f} ¬± {std_reward_best:.6f}")
    print("="*40)

if __name__ == "__main__":
    main()
